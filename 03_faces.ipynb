{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirui-Brian/Biometrics_Systems_Security/blob/main/03_faces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Face** Recognition\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Simple implementation of face recognition leveraging [ArcFace](https://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.pdf).  \n",
        "\n",
        "Language: Python 3  \n",
        "\n",
        "Needed libraries:\n",
        "* NumPy (https://numpy.org/)\n",
        "* matplotlib (https://matplotlib.org/)\n",
        "* OpenCV (https://opencv.org/)\n",
        "* ArcFace implementation (https://github.com/mobilesec/arcface-tensorflowlite), installed through PyPI (https://pypi.org/project/pip/).\n"
      ],
      "metadata": {
        "id": "W2h79Hrpn2yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Needed libraries and files"
      ],
      "metadata": {
        "id": "ED_n3gjLCnzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download of pre-trained face detection models\n",
        "!pip install gdown\n",
        "!gdown 1SLPTyomKCNF7j98Vf0L00fWZy_uiKsHn\n",
        "!gdown 1r22yYX5sqor0vgqXoJcfdRkApji8w-7G\n",
        "!gdown 1vXBmymfKmJp1B8hjyjxJKUWfK686ptXX"
      ],
      "metadata": {
        "id": "C7ILncDjboKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ArcFace instalation and auxiliary libraries\n",
        "# Reference: https://github.com/mobilesec/arcface-tensorflowlite\n",
        "!pip install arcface"
      ],
      "metadata": {
        "id": "bUw-J0qHnFXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# other imported libraries\n",
        "import numpy as np\n",
        "print('NumPy version', np.__version__)\n",
        "\n",
        "import matplotlib as plt\n",
        "print('Matplotlib version', plt.__version__)\n",
        "\n",
        "import cv2\n",
        "print('OpenCV version', cv2.__version__)"
      ],
      "metadata": {
        "id": "o06jf3svCq4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------\n",
        "## Face acquisition"
      ],
      "metadata": {
        "id": "MJkFzjVKCTRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auxiliary function\n",
        "* <code>_capture_png</code>: to open the webcam and capture a PNG file.\n"
      ],
      "metadata": {
        "id": "pV0epaDNFs7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "# javascript-based function to capture pictures from the webcam in PNG\n",
        "def _capture_png(filename):\n",
        "  js = Javascript('''\n",
        "    async function capture() {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture PNG';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      return canvas.toDataURL('image/png');\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  data = eval_js('capture()')\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)"
      ],
      "metadata": {
        "id": "liVLCnlEFsCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests capturing PNG file from the webcam\n",
        "# webcam test\n",
        "image_filepath = '/content/capture.png'\n",
        "_capture_png(image_filepath)\n",
        "\n",
        "image = cv2.imread(image_filepath)\n",
        "plt.pyplot.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.pyplot.show()"
      ],
      "metadata": {
        "id": "dG1OHgugG9BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main functions"
      ],
      "metadata": {
        "id": "WHTTAYzXGaEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Acquires an image that might contain a face from the given file path.\n",
        "# The image is acquired with three color channels (BGR).\n",
        "# Parameters\n",
        "# file_path: The path to the image file containing the face.\n",
        "# view: TRUE if loaded image must be shown, FALSE otherwise.\n",
        "def acquire_from_file(file_path, view=False):\n",
        "    # reads the image from the given file path\n",
        "    # and returns it\n",
        "    image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # shows the read image, if it is the case\n",
        "    if view:\n",
        "        plt.pyplot.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        plt.pyplot.title('Face acquisition')\n",
        "        plt.pyplot.show()\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "nh2RQv0VGj8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the file-based face acquisition function\n",
        "image_filepath = '/content/capture.png'\n",
        "image = acquire_from_file(image_filepath, view=True)\n",
        "assert image.size > 0"
      ],
      "metadata": {
        "id": "1Pwc2mUTGzGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acquires an image that might contain a face from the webcam.\n",
        "# The image is acquired with three color channels (BGR).\n",
        "# Parameters\n",
        "# image_filepath: The path to the image file that will store the captured PNG\n",
        "# view: TRUE if captured image must be shown, FALSE otherwise.\n",
        "def acquire_from_cam(image_filepath='/content/capture.png', view=False):\n",
        "  # webcam capture\n",
        "  _capture_png(image_filepath)\n",
        "\n",
        "  # acquisition from file\n",
        "  return acquire_from_file(image_filepath, view=view)"
      ],
      "metadata": {
        "id": "RHEmrE-UICqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the webcam-based face acquisition function\n",
        "image = acquire_from_cam(view=True)\n",
        "assert image.size > 0"
      ],
      "metadata": {
        "id": "RJl1ncA-JSAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "## Face enhancement"
      ],
      "metadata": {
        "id": "AfmHRRVFJeoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auxiliary function\n",
        "* <code>_rotate_face</code>: to rotate the given face image keeping eyes at the same level."
      ],
      "metadata": {
        "id": "Yu_Ox6BGKdTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rotates the given <image> and reference face rectangle <face_rect> CCW,\n",
        "# obeying the given <rad_angle>.\n",
        "# returns the rotated image and new face rectangle (x, y, w, h).\n",
        "def _rotate_face(image, face_rect, rad_angle):\n",
        "    # rotation matrix\n",
        "    sine = np.sin(rad_angle)\n",
        "    cosine = np.cos(rad_angle)\n",
        "\n",
        "    rot_mat = np.zeros((3, 3))\n",
        "    rot_mat[0, 0] = cosine\n",
        "    rot_mat[0, 1] = -sine\n",
        "    rot_mat[1, 0] = sine\n",
        "    rot_mat[1, 1] = cosine\n",
        "    rot_mat[2, 2] = 1.0\n",
        "\n",
        "    # rotates the image borders\n",
        "    rot_border = np.array([(0, 0), (0, image.shape[0]), (image.shape[1], 0), (image.shape[1], image.shape[0])])\n",
        "    rot_border = cv2.perspectiveTransform(np.float32([rot_border]), rot_mat)[0]\n",
        "    rot_w = int(round(np.max(rot_border[:, 0]) - np.min(rot_border[:, 0])))\n",
        "    rot_h = int(round(np.max(rot_border[:, 1]) - np.min(rot_border[:, 1])))\n",
        "\n",
        "    # translation added to the rotation matrix to compensate for negative points\n",
        "    rot_mat[0, 2] = rot_mat[0, 2] - np.min(rot_border[:, 0])\n",
        "    rot_mat[1, 2] = rot_mat[1, 2] - np.min(rot_border[:, 1])\n",
        "\n",
        "    # rotates the given image\n",
        "    rot_image = cv2.warpPerspective(image, rot_mat, (rot_w, rot_h))\n",
        "\n",
        "    # rotates the given face rectangle\n",
        "    x, y, w, h = face_rect\n",
        "    rot_face_rect = np.array([(x, y), (x + w, y), (x, y + h), (x + w, y + h)])\n",
        "    rot_face_rect = cv2.perspectiveTransform(np.float32([rot_face_rect]), rot_mat)[0]\n",
        "\n",
        "    # computes a new non-rotated face rectangle containing the rotated one\n",
        "    new_face_rect = np.min(rot_face_rect[:, 0]), np.min(rot_face_rect[:, 1]), np.max(\n",
        "        rot_face_rect[:, 0]), np.max(rot_face_rect[:, 1])\n",
        "\n",
        "    new_face_rect = int(round(new_face_rect[0])), int(round(new_face_rect[1])), int(\n",
        "        round(new_face_rect[2] - new_face_rect[0])), int(round(new_face_rect[3] - new_face_rect[1]))\n",
        "\n",
        "    # returns the rotated image and new face rectangle\n",
        "    return rot_image, new_face_rect"
      ],
      "metadata": {
        "id": "t145N80qKzvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the image rotation\n",
        "# original toy-case image with synthetic face (white circle)\n",
        "# and detection (green rectangle)\n",
        "original_image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "cv2.circle(original_image, (20, 50), 10, (255, 255, 255), -1)\n",
        "cv2.circle(original_image, (16, 48), 2, (0, 0, 0), -1)\n",
        "cv2.circle(original_image, (24, 48), 2, (0, 0, 0), -1)\n",
        "cv2.line(original_image, (16, 55), (24, 55), (0, 0, 0), 1)\n",
        "\n",
        "face_rect = 10, 40, 20, 20\n",
        "face_rect_p1 = face_rect[0:2]\n",
        "face_rect_p2 = (face_rect[0] + face_rect[2], face_rect[1] + face_rect[3])\n",
        "\n",
        "detection_image = original_image.copy()\n",
        "cv2.rectangle(detection_image, face_rect_p1, face_rect_p2, (0, 255, 0), 1)\n",
        "\n",
        "plt.pyplot.imshow(cv2.cvtColor(detection_image, cv2.COLOR_BGR2RGB))\n",
        "plt.pyplot.title('Original image and detection')\n",
        "plt.pyplot.show()\n",
        "\n",
        "# rotates the image by 90 degrees CCW\n",
        "rot_image, new_face_rect = _rotate_face(original_image, face_rect, np.pi / 2)\n",
        "new_face_rect_p1 = new_face_rect[0:2]\n",
        "new_face_rect_p2 = (new_face_rect[0] + new_face_rect[2],\n",
        "                    new_face_rect[1] + new_face_rect[3])\n",
        "\n",
        "detection_rot_image = rot_image.copy()\n",
        "cv2.rectangle(detection_rot_image, new_face_rect_p1, new_face_rect_p2,(0, 255, 0), 1)\n",
        "\n",
        "plt.pyplot.imshow(cv2.cvtColor(detection_rot_image, cv2.COLOR_BGR2RGB))\n",
        "plt.pyplot.title('Rotated image and detection (90 degrees CCW)')\n",
        "plt.pyplot.show()\n",
        "\n",
        "# rotates the image by 45 degrees CCW\n",
        "rot_image, new_face_rect = _rotate_face(original_image, face_rect, np.pi / 4)\n",
        "new_face_rect_p1 = new_face_rect[0:2]\n",
        "new_face_rect_p2 = (new_face_rect[0] + new_face_rect[2],\n",
        "                    new_face_rect[1] + new_face_rect[3])\n",
        "\n",
        "detection_rot_image = rot_image.copy()\n",
        "cv2.rectangle(detection_rot_image, new_face_rect_p1, new_face_rect_p2,(0, 255, 0), 1)\n",
        "\n",
        "plt.pyplot.imshow(cv2.cvtColor(detection_rot_image, cv2.COLOR_BGR2RGB))\n",
        "plt.pyplot.title('Rotated image and detection (45 degrees CCW)')\n",
        "plt.pyplot.show()"
      ],
      "metadata": {
        "id": "0kZrg9aILxg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main steps\n",
        "* <code>_01_preprocess</code>: to pre-process the given image, resizing and bringing it to grayscale.\n",
        "* <code>_02_detect_face</code>: to detect a face on the given image.\n",
        "* <code>_03_align_face</code>: to align the face so that both eyes are leveled.\n",
        "* <code>_04_extract_face</code>: to crop and normalize the colors of the detected face."
      ],
      "metadata": {
        "id": "LBYuvy0UZgyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesses the given <image> for further face detection.\n",
        "# Provide <view> as True if you want to see the result of computations.\n",
        "# Returns the preprocessed image.\n",
        "def _01_preprocess(image, image_width=640, view=False):\n",
        "    # makes the image grayscale, if it is still colored\n",
        "    if len(image.shape) > 2 and image.shape[2] > 1:  # more than one channel?\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # resizes the image to present a width of <image_width> pixels,\n",
        "    # keeping the original aspect ratio\n",
        "    aspect_ratio = float(image.shape[1]) / image.shape[0]\n",
        "    height = int(round(image_width / aspect_ratio))\n",
        "    image = cv2.resize(image, (image_width, height))\n",
        "\n",
        "    # shows the obtained image, if it is the case\n",
        "    if view:\n",
        "      plt.pyplot.imshow(image, cmap='gray')\n",
        "      plt.pyplot.title('Pre-processed image')\n",
        "      plt.pyplot.show()\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "ClrIEsqrYl2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the pre-processing of a given image\n",
        "image = acquire_from_file('/content/capture.png', view=True)\n",
        "pp_image = _01_preprocess(image, view=True)"
      ],
      "metadata": {
        "id": "Epor2yhRaTDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detects the largest face over the given gray-scaled image.\n",
        "# Provide <view> as True if you want to see the result of computations.\n",
        "# Returns the rectangle (x, y, w, h) of the detected face.\n",
        "def _02_detect_face(gs_image,\n",
        "                    model_path='/content/violajones_haarcascade_frontalface_default.xml',\n",
        "                    view=False):\n",
        "    # detects faces on the given image with the Viola-Jones detector\n",
        "    vj_face_detector = cv2.CascadeClassifier(model_path)\n",
        "    face_boxes = vj_face_detector.detectMultiScale(gs_image)\n",
        "\n",
        "    # if there are no faces, returns None\n",
        "    if len(face_boxes) == 0:\n",
        "        return None\n",
        "\n",
        "    # else...\n",
        "    # takes the largest face among the detected ones\n",
        "    # TODO detecting more faces can be added here\n",
        "    x, y, w, h = 0, 0, 0, 0\n",
        "    size = 0\n",
        "    for face in face_boxes:\n",
        "        if face[2] * face[3] > size:\n",
        "            x, y, w, h = face\n",
        "            size = w * h\n",
        "\n",
        "    # show the obtained face, if it is the case\n",
        "    if view:\n",
        "        view_image = cv2.cvtColor(gs_image, cv2.COLOR_GRAY2BGR)\n",
        "        cv2.rectangle(view_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        plt.pyplot.imshow(cv2.cvtColor(view_image, cv2.COLOR_BGR2RGB))\n",
        "        plt.pyplot.title('Detected face')\n",
        "        plt.pyplot.show()\n",
        "\n",
        "    return x, y, w, h"
      ],
      "metadata": {
        "id": "dNdCbQXWa41B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detecs a face on the given image\n",
        "face = _02_detect_face(pp_image, view=True)\n",
        "print('Detected face:', face)"
      ],
      "metadata": {
        "id": "ZQm6NX6Ldpfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms the given gray-scaled image <gs_image> according to the given\n",
        "# face position <face_rect>, making it up-face aligned with the horizontal line.\n",
        "# Provide <view> as True if you want to see the result of computations.\n",
        "# Returns the transformed image and new rectangle (x, y, w, h)\n",
        "# of the aligned face.\n",
        "def _03_align_face(gs_image, face_rect,\n",
        "                   model_path='/content/violajones_haarcascade_eye.xml',\n",
        "                   view=False):\n",
        "    # focus on the image region containing the face\n",
        "    x, y, w, h = face_rect\n",
        "    face_image = gs_image[y:y + h, x:x + w]\n",
        "\n",
        "    # detects eyes on the face with Viola-Jones detector\n",
        "    vj_eyes_detector = cv2.CascadeClassifier(model_path)\n",
        "    eye_boxes = vj_eyes_detector.detectMultiScale(face_image)\n",
        "\n",
        "    # if eyes were detected...\n",
        "    if len(eye_boxes) == 2:\n",
        "        # rotates the face in a way that eyes are aligned\n",
        "        # in the horizontal position\n",
        "        x1, y1, w1, h1, = eye_boxes[0]  # eye 1\n",
        "        x2, y2, w2, h2, = eye_boxes[1]  # eye 2\n",
        "\n",
        "        if x1 < x2:\n",
        "            xc1 = x1 + w1 / 2.0  # right eye, mirrored on the left\n",
        "            yc1 = y1 + h1 / 2.0\n",
        "\n",
        "            xc2 = x2 + w2 / 2.0  # left eye, mirrored on the right\n",
        "            yc2 = y2 + h2 / 2.0\n",
        "\n",
        "        else:\n",
        "            xc2 = x1 + w1 / 2.0  # left eye, mirrored on the right\n",
        "            yc2 = y1 + h1 / 2.0\n",
        "\n",
        "            xc1 = x2 + w2 / 2.0  # right eye, mirrored on the right\n",
        "            yc1 = y2 + h2 / 2.0\n",
        "\n",
        "        # angle between eyes\n",
        "        angle = np.arctan2(yc2 - yc1, xc2 - xc1)\n",
        "\n",
        "        # obtains the aligned image and new face rectangle\n",
        "        gs_image, face_rect = _rotate_face(gs_image, face_rect, -angle)\n",
        "\n",
        "    # shows the aligned face, if it is the case\n",
        "    if view:\n",
        "        x, y, w, h = face_rect\n",
        "        view_image = cv2.cvtColor(gs_image, cv2.COLOR_GRAY2BGR)\n",
        "        cv2.rectangle(view_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        plt.pyplot.imshow(cv2.cvtColor(view_image, cv2.COLOR_BGR2RGB))\n",
        "        plt.pyplot.title('Aligned face')\n",
        "        plt.pyplot.show()\n",
        "\n",
        "    return gs_image, face_rect"
      ],
      "metadata": {
        "id": "Z4bA0b9NfE27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aligns the detected face\n",
        "al_image, al_face = _03_align_face(pp_image, face, view=True)\n",
        "print('Aligned face:', al_face)"
      ],
      "metadata": {
        "id": "wQDDaYSefzde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crops and normalizes the face contained in the given gray-scaled image\n",
        "# <gs_image>, following the provided face rectangle <face_rect>.\n",
        "# Provide <view> as True if you want to see the result of computations.\n",
        "# Returns the extracted face, ready for description (feature extraction).\n",
        "def _04_extract_face(gs_image, face_rect, face_size=256, view=False):\n",
        "    x, y, w, h = face_rect\n",
        "    cx = int(round(x + w / 2.0))\n",
        "    cy = int(round(y + h / 2.0))\n",
        "    r = int(round(max(w, h) / 2.0))\n",
        "\n",
        "    face_image = gs_image[\n",
        "                 max(0, cy - r):min(cy + r + 1, gs_image.shape[0]),\n",
        "                 max(0, cx - r):min(cx + r + 1, gs_image.shape[1])]  # squared face\n",
        "    if len(face_image) > 0:\n",
        "        face_image = cv2.resize(face_image, (face_size, face_size))  # face in normalized size\n",
        "        face_image = cv2.equalizeHist(face_image)  # color histogram normalization\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    if view:\n",
        "      plt.pyplot.imshow(face_image, cmap='gray')\n",
        "      plt.pyplot.title('Extracted face')\n",
        "      plt.pyplot.show()\n",
        "\n",
        "    return face_image"
      ],
      "metadata": {
        "id": "zceO8nsjg--9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the extraction of face\n",
        "face_image = _04_extract_face(al_image, al_face, view=True)"
      ],
      "metadata": {
        "id": "VmqIDAvQhkVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "W1tSxydyiqyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhances the given image, returning the normalized version of the largest\n",
        "# face depicted within it.\n",
        "# Provide <view> as True if you want to see the results of computations.\n",
        "# Returns the normalized face image, useful for description (feature extraction)\n",
        "# or None, if no face was detected.\n",
        "def enhance(image, view=False):\n",
        "    # pre-processes the given image\n",
        "    pp_image = _01_preprocess(image, view=view)\n",
        "\n",
        "    # detects a face in the given image\n",
        "    face_rect = _02_detect_face(pp_image, view=view)\n",
        "\n",
        "    if face_rect is not None:\n",
        "        # aligns the obtained face\n",
        "        aligned_image, aligned_face = _03_align_face(pp_image, face_rect, view=view)\n",
        "\n",
        "        # extracts and returns the detected face\n",
        "        return _04_extract_face(aligned_image, aligned_face, view=view)\n",
        "\n",
        "    # no face was found\n",
        "    return None"
      ],
      "metadata": {
        "id": "o-HyRFjwitcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the enhancement function\n",
        "image = acquire_from_file('/content/capture.png', view=True)\n",
        "face_image = enhance(image, view=True)"
      ],
      "metadata": {
        "id": "ozY8dBkijNwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------\n",
        "## Face description"
      ],
      "metadata": {
        "id": "dhdbE-jaaSta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "p_AiY_ExvYi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from arcface import ArcFace\n",
        "\n",
        "# Describes the given normalized face image <norm_face> with ArcFace.\n",
        "# Returns the obtained feature vector.\n",
        "def describe(norm_face, arc_face_model='/content/model.tflite'):\n",
        "    face_descriptor = ArcFace.ArcFace(model_path=arc_face_model)\n",
        "    feature_vector = face_descriptor.calc_emb(norm_face)\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "GNhYKvNAvX1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the face description\n",
        "description = describe(face_image)\n",
        "print('Feature vector size:', description.shape)\n",
        "print('Feature vector:', description)"
      ],
      "metadata": {
        "id": "ixQN2diyvvSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "## Face matching"
      ],
      "metadata": {
        "id": "K10AH52Our1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "Vxu4MSyaW40j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from arcface import ArcFace\n",
        "\n",
        "# Matches the given feature vectors <description_1> and <description_2>.\n",
        "# Returns the distance between them, expressing how likely they are of\n",
        "# representing the same face.\n",
        "# The distance is a positive real number.\n",
        "def match(description_1, description_2, arc_face_model='/content/model.tflite'):\n",
        "    face_descriptor = ArcFace.ArcFace(model_path=arc_face_model)\n",
        "    distance = face_descriptor.get_distance_embeddings(description_1,\n",
        "                                                       description_2)\n",
        "    return distance"
      ],
      "metadata": {
        "id": "Ol1R6X_IwM9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests the matching function\n",
        "# same face compared to itself (same capture)\n",
        "dist = match(description, description)\n",
        "print('Distance:' , dist)\n",
        "assert dist == 0\n"
      ],
      "metadata": {
        "id": "v08j2AloyAmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamicaly acquired faces\n",
        "image_1 = acquire_from_cam(view=True)\n",
        "image_2 = acquire_from_cam(view=True)\n",
        "\n",
        "face_1 = enhance(image_1, view=True)\n",
        "face_2 = enhance(image_2, view=True)\n",
        "\n",
        "description_1 = describe(face_1)\n",
        "description_2 = describe(face_2)\n",
        "dist = match(description_1, description_2)\n",
        "print('Distance:' , dist)"
      ],
      "metadata": {
        "id": "9zqv_WLjzVwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "________\n",
        "## Exercise\n",
        "\n",
        "Download the 16 images with faces below and study their distances according to ArcFace's description."
      ],
      "metadata": {
        "id": "pjP2P_Gzzm8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download of 16 images\n",
        "!gdown 1Y5rQb6HGgpxlnj6LNo6u_lUctr6dgGwA\n",
        "!gdown 1yVBf2s779yAS5cm22AC6IJ1fnU-zsMOZ\n",
        "!gdown 1UQJqcgJKYLKJWCfi6gFae_4npa95M-LS\n",
        "!gdown 1cAi4lCnsuXM2Lcd7gbhJZx_GoFusfN1i\n",
        "!gdown 1FizW_0tCc5ysbJyPr3_hXCZZOTxvXi3O\n",
        "!gdown 1a4pWT7423G3HLt8mDUtr79nYjR9EBOWO\n",
        "!gdown 1Zw2SGR8li3k7Oi3PPlAlUvUaRNLlQ2dc\n",
        "!gdown 1ULuUlNIdnN7hkSSJti7psFYH65VQVDGt\n",
        "!gdown 1hO82av0RUJO1ECK2scm6miAETjCg-OIs\n",
        "!gdown 1_Aca0L5QcioA6Uivss7KOHivQLN7uN_z\n",
        "!gdown 1VtFelqyqYSLl4MBIOj5c2ov00Auesjai\n",
        "!gdown 1sCxXJqtEu3SOtxX4JiPxvp4bYQ4Ppt6G\n",
        "!gdown 1nBLwFuUf8GHm5cgHojtCKjrBcKH4BDgR\n",
        "!gdown 12MI79nkO8IZfP9uW8pNsNgRkz4gEV19_\n",
        "!gdown 1HRZX8BaAimfZj0NV2bfR8FoSuddJQOZn\n",
        "!gdown 1XrqSXNKCPyruyhIGWvnHbZlhIik0okkC"
      ],
      "metadata": {
        "id": "Q867JC5z4XTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtained faces\n",
        "image_0101 = acquire_from_file('/content/0101.jpg', view=True)\n",
        "image_0102 = acquire_from_file('/content/0102.jpg', view=True)\n",
        "image_0201 = acquire_from_file('/content/0201.jpg', view=True)\n",
        "image_0202 = acquire_from_file('/content/0202.jpg', view=True)\n",
        "image_0301 = acquire_from_file('/content/0301.jpg', view=True)\n",
        "image_0302 = acquire_from_file('/content/0302.jpg', view=True)\n",
        "image_0401 = acquire_from_file('/content/0401.jpg', view=True)\n",
        "image_0402 = acquire_from_file('/content/0402.jpg', view=True)\n",
        "image_0501 = acquire_from_file('/content/0501.jpg', view=True)\n",
        "image_0502 = acquire_from_file('/content/0502.jpg', view=True)\n",
        "image_0601 = acquire_from_file('/content/0601.jpg', view=True)\n",
        "image_0602 = acquire_from_file('/content/0602.jpg', view=True)\n",
        "image_0701 = acquire_from_file('/content/0701.jpg', view=True)\n",
        "image_0702 = acquire_from_file('/content/0702.jpg', view=True)\n",
        "image_0801 = acquire_from_file('/content/0801.jpg', view=True)\n",
        "image_0802 = acquire_from_file('/content/0802.jpg', view=True)"
      ],
      "metadata": {
        "id": "BaazR6E-Djcy",
        "outputId": "e766e854-0a85-4894-867e-5f8ef31e00f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'acquire_from_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-231f1c44d49d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# obtained faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_0101\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/0101.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage_0102\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/0102.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_0201\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/0201.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_0202\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/0202.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'acquire_from_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add your study here\n"
      ],
      "metadata": {
        "id": "WbadJsHUFQpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}